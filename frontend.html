<!DOCTYPE html>
<html>
<head>
    <title>Live Audio Chat</title>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; padding: 50px; }
        #chatButton { padding: 20px 40px; font-size: 18px; border: none; border-radius: 10px; cursor: pointer; }
        .recording { background-color: #ff4444; color: white; }
        .stopped { background-color: #44ff44; color: black; }
        #status { margin-top: 20px; font-size: 16px; }
        #debug { margin-top: 10px; font-size: 12px; color: #666; }
        .header { margin-bottom: 30px; }
        .interview-mode { 
            background: #f0f8ff; 
            padding: 15px; 
            border-radius: 10px; 
            margin-bottom: 20px;
            border-left: 4px solid #007acc;
        }
        .back-button {
            position: absolute;
            top: 20px;
            left: 20px;
            padding: 10px 20px;
            background: #ddd;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            text-decoration: none;
            color: #333;
        }
        .back-button:hover {
            background: #ccc;
        }
    </style>
</head>
<body>
    <a href="index.html" class="back-button">‚Üê Back to Menu</a>
    
    <div class="header">
        <h1>üé§ Live Audio Chat</h1>
        <div class="interview-mode" id="interviewMode">
            Loading interview mode...
        </div>
    </div>
    
    <button id="chatButton" class="stopped">Start Conversation</button>
    <div id="status">Click to start talking</div>
    <div id="debug">Debug info will appear here</div>

    <script>
        let ws = null;
        let recordingAudioContext = null;
        let playbackAudioContext = null;
        let processor = null;
        let source = null;
        let isRecording = false;
        let stream = null;
        let selectedPrompt = null;
        
        // Audio playback queue
        let audioQueue = [];
        let isPlaying = false;
        let nextPlayTime = 0;
        
        const button = document.getElementById('chatButton');
        const status = document.getElementById('status');
        const debug = document.getElementById('debug');
        
        button.addEventListener('click', toggleRecording);
        
        function debugLog(message) {
            console.log(message);
            debug.textContent = message;
        }
        
        // Get prompt from URL parameters
        function getPromptFromUrl() {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get('prompt');
        }
        
        // Load and display selected prompt
        async function loadPromptInfo() {
            const promptKey = getPromptFromUrl();
            if (!promptKey) {
                document.getElementById('interviewMode').innerHTML = `
                    <strong>General Chat Mode</strong><br>
                    No specific interview mode selected
                `;
                return null;
            }
            
            try {
                const response = await fetch('prompts.json');
                const prompts = await response.json();
                
                if (prompts[promptKey]) {
                    const prompt = prompts[promptKey];
                    document.getElementById('interviewMode').innerHTML = `
                        <strong>${prompt.name}</strong><br>
                        ${prompt.description}<br>
                        <em>${prompt.welcome_message}</em>
                    `;
                    return promptKey;
                } else {
                    throw new Error('Prompt not found');
                }
            } catch (error) {
                console.error('Error loading prompt:', error);
                document.getElementById('interviewMode').innerHTML = `
                    <strong>Error</strong><br>
                    Could not load interview mode. Using general chat.
                `;
                return null;
            }
        }
        
        async function toggleRecording() {
            if (!isRecording) {
                await startRecording();
            } else {
                stopRecording();
            }
        }
        
        async function startRecording() {
            try {
                debugLog('Connecting to WebSocket...');
                
                // Reset audio queue
                audioQueue = [];
                isPlaying = false;
                nextPlayTime = 0;
                
                // Connect to WebSocket server
                ws = new WebSocket('ws://localhost:8765');
                
                ws.onopen = () => {
                    status.textContent = 'Connected to server';
                    debugLog('WebSocket connected');
                    
                    // Send prompt configuration if available
                    if (selectedPrompt) {
                        ws.send(JSON.stringify({
                            type: 'prompt_selection',
                            prompt: selectedPrompt
                        }));
                        debugLog(`Sent prompt selection: ${selectedPrompt}`);
                    }
                };
                
                ws.onmessage = async (event) => {
                    // Handle text messages (like confirmations)
                    if (typeof event.data === 'string') {
                        try {
                            const message = JSON.parse(event.data);
                            if (message.type === 'prompt_configured') {
                                debugLog(`Interview mode configured: ${message.prompt}`);
                            }
                        } catch (e) {
                            // Not JSON, probably a text message
                            debugLog(`Server message: ${event.data}`);
                        }
                        return;
                    }
                    
                    // Handle audio data
                    debugLog(`Received audio chunk: ${event.data.byteLength} bytes`);
                    if (event.data instanceof ArrayBuffer) {
                        await queueAudioChunk(event.data);
                    } else if (event.data instanceof Blob) {
                        const buffer = await event.data.arrayBuffer();
                        await queueAudioChunk(buffer);
                    }
                };
                
                ws.onerror = (error) => {
                    status.textContent = 'WebSocket error';
                    debugLog('WebSocket error: ' + error);
                    console.error('WebSocket error:', error);
                };
                
                debugLog('Requesting microphone access...');
                
                // Get microphone access - use 16kHz to match Gemini expectations
                stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: { 
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: false
                    } 
                });
                
                debugLog('Microphone access granted, setting up raw PCM capture...');
                
                // Create separate audio context for recording at 16kHz
                recordingAudioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: 16000
                });
                
                source = recordingAudioContext.createMediaStreamSource(stream);
                
                // Use ScriptProcessor for raw PCM data
                processor = recordingAudioContext.createScriptProcessor(4096, 1, 1);
                
                processor.onaudioprocess = (event) => {
                    if (ws && ws.readyState === WebSocket.OPEN) {
                        const inputBuffer = event.inputBuffer;
                        const channelData = inputBuffer.getChannelData(0);
                        
                        // Convert Float32 samples to Int16 PCM
                        const pcmSamples = new Int16Array(channelData.length);
                        for (let i = 0; i < channelData.length; i++) {
                            const sample = Math.max(-1, Math.min(1, channelData[i]));
                            pcmSamples[i] = sample * 32767;
                        }
                        
                        // Send raw PCM bytes
                        ws.send(pcmSamples.buffer);
                    }
                };
                
                // Connect the audio processing chain
                source.connect(processor);
                processor.connect(recordingAudioContext.destination);
                
                // Create separate audio context for playback at higher sample rate
                playbackAudioContext = new (window.AudioContext || window.webkitAudioContext)();
                
                // Resume playback context if suspended
                if (playbackAudioContext.state === 'suspended') {
                    await playbackAudioContext.resume();
                }
                
                isRecording = true;
                button.textContent = 'Stop Conversation';
                button.className = 'recording';
                status.textContent = 'Recording... Speak now!';
                debugLog('Ready for streaming audio conversation');
                
            } catch (error) {
                console.error('Error starting recording:', error);
                status.textContent = 'Error: ' + error.message;
                debugLog('Error: ' + error.message);
            }
        }
        
        async function queueAudioChunk(audioData) {
            try {
                // Ensure playback context is ready
                if (!playbackAudioContext || playbackAudioContext.state === 'closed') {
                    playbackAudioContext = new (window.AudioContext || window.webkitAudioContext)();
                }
                
                if (playbackAudioContext.state === 'suspended') {
                    await playbackAudioContext.resume();
                }
                
                // Convert raw PCM data to audio buffer
                const samples = new Int16Array(audioData);
                if (samples.length === 0) return;
                
                // Convert Int16 PCM to Float32 for Web Audio API
                const floatSamples = new Float32Array(samples.length);
                for (let i = 0; i < samples.length; i++) {
                    floatSamples[i] = samples[i] / 32768.0;
                }
                
                // Create audio buffer at 24kHz (Gemini's output sample rate)
                const audioBuffer = playbackAudioContext.createBuffer(
                    1,                    // mono
                    floatSamples.length, // number of samples
                    24000                // 24kHz sample rate
                );
                
                audioBuffer.getChannelData(0).set(floatSamples);
                
                // Add to queue
                audioQueue.push(audioBuffer);
                
                debugLog(`Queued audio chunk: ${(floatSamples.length / 24000).toFixed(3)}s (${audioQueue.length} in queue)`);
                
                // Start playing if not already playing
                if (!isPlaying) {
                    playNextChunk();
                }
                
            } catch (error) {
                console.error('Error queueing audio chunk:', error);
                debugLog(`Audio queue error: ${error.message}`);
            }
        }
        
        async function playNextChunk() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                debugLog('Audio queue empty - playback stopped');
                return;
            }
            
            isPlaying = true;
            const audioBuffer = audioQueue.shift();
            
            try {
                // Create audio source
                const sourceNode = playbackAudioContext.createBufferSource();
                sourceNode.buffer = audioBuffer;
                
                // Add gain control
                const gainNode = playbackAudioContext.createGain();
                gainNode.gain.value = 0.8;
                
                sourceNode.connect(gainNode);
                gainNode.connect(playbackAudioContext.destination);
                
                // Calculate when to start this chunk
                const currentTime = playbackAudioContext.currentTime;
                const startTime = Math.max(currentTime, nextPlayTime);
                
                // Schedule the next chunk to start when this one ends
                const duration = audioBuffer.length / audioBuffer.sampleRate;
                nextPlayTime = startTime + duration;
                
                // Play the chunk
                sourceNode.start(startTime);
                
                debugLog(`Playing chunk: ${duration.toFixed(3)}s (${audioQueue.length} remaining)`);
                
                // When this chunk ends, play the next one
                sourceNode.onended = () => {
                    playNextChunk();
                };
                
            } catch (error) {
                console.error('Error playing audio chunk:', error);
                debugLog(`Playback error: ${error.message}`);
                // Try to continue with next chunk
                playNextChunk();
            }
        }
        
        function stopRecording() {
            debugLog('Stopping recording...');
            
            // Clear audio queue
            audioQueue = [];
            isPlaying = false;
            nextPlayTime = 0;
            
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            
            if (source) {
                source.disconnect();
                source = null;
            }
            
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                stream = null;
            }
            
            if (recordingAudioContext && recordingAudioContext.state !== 'closed') {
                recordingAudioContext.close();
                recordingAudioContext = null;
            }
            
            if (ws) {
                ws.close();
                ws = null;
            }
            
            isRecording = false;
            button.textContent = 'Start Conversation';
            button.className = 'stopped';
            status.textContent = 'Click to start talking';
            debugLog('Recording stopped');
        }
        
        // Initialize page
        window.addEventListener('load', async () => {
            selectedPrompt = await loadPromptInfo();
        });
    </script>
</body>
</html> 